{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import scipy\n",
    "import scipy.optimize\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal:\n",
    "Given (user, music, format(optional)) tuple, predict the rating that the user will give to the music."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful fields:\n",
    "# |name       | possible value  | analysis\n",
    "# \"overall\":    1 - 5 (int)\n",
    "# \"verified\":   True / False      (Don't know meaning yet)\n",
    "# \"reviewerID\": \"A1SJL3JBBILJ66\"\n",
    "# \"asin\": \"     B0018CGCR4\"       (music ID)\n",
    "# \"format\":     \" MP3 Music\"      86.44%\n",
    "#               \" Audio CD\"       6.37%\n",
    "#               \"\" (undeclared)   6.95%\n",
    "#               \" Vinyl\"          .2%\n",
    "#               (others)          <.04%\n",
    "# \"reviewText\": \"THANK YOU\"       .09% users doesn't provide reviewText, indcicate as \"\"\n",
    "# \"summary\":    \"Five Stars\"      .002% users doesn't provide summary, indcicate as \"\"\n",
    "# \"image\":      0 (int)           .107% users provide image\n",
    "#                                 indicate number of images provided in the review\n",
    "# \"vote\":       0 (int)           4.48% reviewers are voted by others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "5\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "6\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "10\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Reading training file')\n",
    "\n",
    "f = open(\"./train.json\", 'rt', encoding=\"utf8\")\n",
    "\n",
    "data = [f.readline()]\n",
    "data = data.replace(', \"verified\": true, \"', ', \"verified\": True, \"')\n",
    "data = data.replace(', \"verified\": false, \"', ', \"verified\": False, \"')\n",
    "data = eval(data)\n",
    "\n",
    "parsed_data = open(\"pdata.json\", 'w')\n",
    "for d in data:\n",
    "    # unused fields\n",
    "    d.pop('reviewTime', None)\n",
    "    d.pop('reviewerName', None)\n",
    "    d.pop('unixReviewTime', None)\n",
    "    \n",
    "    # overall\n",
    "    d['overall'] = int(d['overall'])\n",
    "    \n",
    "    # style\n",
    "    if 'style' in d:\n",
    "        d['format'] = d['style']['Format:']\n",
    "        d.pop('style', None)\n",
    "    else:\n",
    "        d['format'] = \"\"\n",
    "    \n",
    "    # vote\n",
    "    if not 'vote' in d:\n",
    "        _ += 1\n",
    "        d['vote'] = 0\n",
    "\n",
    "    # image\n",
    "    if 'image' in d:\n",
    "        d['image'] = len(d['image']) \n",
    "    else:\n",
    "        _1 +=1\n",
    "        d['image'] = 0\n",
    "        \n",
    "    if not 'reviewText' in d:\n",
    "        d['reviewText'] = \"\"        \n",
    "\n",
    "    if not 'summary' in d:\n",
    "        d['summary'] = \"\"\n",
    "    parsed_data.write(str(d))\n",
    "\n",
    "parsed_data.close()\n",
    "print('Finished parsing orinal file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constructing train/valid dataset & test dataset \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-f01d3a096eb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'reviewerID'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'asin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "print('constructing train/valid dataset & test dataset ')\n",
    "\n",
    "# constraint when we building the dataset:\n",
    "# ensure each user/data appear at least 4 times\n",
    "f = open(\"./pdata.json\", 'rt', encoding=\"utf8\")\n",
    "\n",
    "us = defaultdict(int)\n",
    "ms = defaultdict(int)\n",
    "data = []\n",
    "for l in f:\n",
    "    data.append(l)\n",
    "    us[l['reviewerID']] += 1\n",
    "    ms[l['asin']] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'true' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5b04ce6236d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# \"reviewText\": \"THANK YOU\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# \"summary\": \"Five Stars\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'style'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Format:'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\" MP3 Music\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'true' is not defined"
     ]
    }
   ],
   "source": [
    "# we can't prove that user will rate a music he'she never listened as the way he/she will rate a listened music.\n",
    "# For example, a user will only rate musics he/she like, so he/she rated every music 5 stars.\n",
    "# It is possible that our model will predict that the user will give a high rating to all musics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "_i = 0\n",
    "\n",
    "for line in f:\n",
    "    u, b, r = line.strip().split(',')\n",
    "    us.add(u)\n",
    "    bs.add(b)\n",
    "    r = int(r)\n",
    "    if _i < 190000:\n",
    "        train.append([u, b, r])\n",
    "        train_u_b[u].add(b)\n",
    "        train_b_u[b].add(u)\n",
    "    elif _i < 200000:\n",
    "        valid.append([u, b, r])\n",
    "        valid_u_b[u].add(b)\n",
    "    else:\n",
    "        break\n",
    "    _i += 1\n",
    "\n",
    "# add the neg samples to valid set\n",
    "for u, b, r in valid:\n",
    "    while True:\n",
    "        _ = random.sample(bs, 1)[0]\n",
    "        if not (_ in train_u_b[u] and _ in valid_u_b[u]):\n",
    "            valid_neg.append([u, _])\n",
    "            break\n",
    "v_size = len(valid_neg) + len(valid_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: base solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# problem 1: base solution\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for u, b, _ in train:\n",
    "    bookCount[b] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 1: Accuracy is: 0.6454\n"
     ]
    }
   ],
   "source": [
    "base_read_set = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    if count < totalRead / 2:\n",
    "        base_read_set.add(i)\n",
    "    count += ic\n",
    "\n",
    "base_read_pred = 0\n",
    "for u, b, _ in valid:\n",
    "    if b in base_read_set:\n",
    "        base_read_pred += 1\n",
    "for u, b in valid_neg:\n",
    "    if not (b in base_read_set):\n",
    "        base_read_pred += 1\n",
    "print(\"Problem 1: Accuracy is: \" + str(base_read_pred / v_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: change threshold of base solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 2: With threshold of 1.65, Accuracy is: 0.64965\n"
     ]
    }
   ],
   "source": [
    "better_read_set = set()\n",
    "count = 0\n",
    "for ic, i in mostPopular:\n",
    "    if count < totalRead / 1.65:\n",
    "        better_read_set.add(i)\n",
    "    count += ic\n",
    "\n",
    "better_read_pred = 0\n",
    "for u, b, _ in valid:\n",
    "    if b in better_read_set:\n",
    "        better_read_pred += 1\n",
    "for u, b in valid_neg:\n",
    "    if not (b in better_read_set):\n",
    "        better_read_pred += 1\n",
    "print(\"Problem 2: With threshold of 1.65, Accuracy is: \" + str(better_read_pred / v_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem3: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 3: With threshold of 0.0277, accuracy is: 0.642\n"
     ]
    }
   ],
   "source": [
    "Jacc_threshold = 0.0277\n",
    "Jacc_errs = 0\n",
    "\n",
    "def Jacc_read(u, b, Jacc_threshold):\n",
    "    s1 = train_u_b[u]\n",
    "    for uu in train_b_u[b]:\n",
    "        sim = Jaccard(s1, train_u_b[uu])\n",
    "        if 1 > sim > Jacc_threshold:\n",
    "            if b in train_u_b[uu]:\n",
    "                return True\n",
    "    return False\n",
    "    \n",
    "for u, b, _ in valid:\n",
    "    if not Jacc_read(u, b, Jacc_threshold):\n",
    "        Jacc_errs += 1\n",
    "\n",
    "for u, b in valid_neg:\n",
    "    if Jacc_read(u, b, Jacc_threshold):\n",
    "        Jacc_errs += 1\n",
    "\n",
    "print(\"Problem 3: With threshold of 0.0277, accuracy is: \" + str(1- Jacc_errs / v_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: based on the popularity of all similar users on a book, \n",
    "# determine if the user have read it or not.\n",
    "\n",
    "def Jacc_pop_read1(u, b, t1=0.022, t2=1.27):\n",
    "    s1 = train_u_b[u]\n",
    "    sims = []\n",
    "    uus = set()\n",
    "    for bb in list(train_u_b[u]):\n",
    "        uus = uus.union(train_b_u[bb])\n",
    "    for uu in list(uus):\n",
    "        s2 = train_u_b[uu]\n",
    "        sim = Jaccard(s1, s2)\n",
    "        if 1 > sim > t1:\n",
    "            sims.append([sim, s2])\n",
    "    _bookf = defaultdict(int)\n",
    "    _f_sum = 0\n",
    "    for _, uu in sims:\n",
    "        for bb in uu:\n",
    "            _bookf[bb] += _ \n",
    "            _f_sum += _\n",
    "    _bookf = [(_bookf[_], _) for _ in _bookf]\n",
    "    _bookf.sort()\n",
    "    _bookf.reverse()\n",
    "    \n",
    "    _f_cnt = 0\n",
    "    for f, bb in _bookf:\n",
    "        if b == bb:\n",
    "            return True\n",
    "        _f_cnt += f\n",
    "        if _f_cnt >= _f_sum / t2:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 4: With Jaccard threshold of 0.022, and popularity threshold of 1.27, \n",
      "Accuracy is: 0.6702\n"
     ]
    }
   ],
   "source": [
    "Jacc_pop_errs = 0\n",
    "for u, b, _ in valid:\n",
    "    if not Jacc_pop_read1(u, b):\n",
    "        Jacc_pop_errs += 1\n",
    "for u, b in valid_neg:\n",
    "    if Jacc_pop_read1(u, b):\n",
    "        Jacc_pop_errs += 1\n",
    "print(\"Problem 4: With Jaccard threshold of 0.022, and popularity threshold of 1.27, \\n\"\n",
    "      \"Accuracy is: \" + str(1 - Jacc_pop_errs / v_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem5:\n",
    "## My kaggle username is Xincheng Shen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate file\n",
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "for l in open(\"./assignment1/pairs_Read.txt\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u, b = l.strip().split('-')\n",
    "    predictions.write( u + '-' + b + (\",1\\n\" if Jacc_pop_read1(u, b) else \",0\\n\"))\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./assignment1/train_Category.json.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "train2, valid2 = [], []\n",
    "_ = 0\n",
    "while True:\n",
    "    if _ < 190000:\n",
    "        train2.append(eval(f.readline()))\n",
    "    elif _ < 200000:\n",
    "        valid2.append(eval(f.readline()))\n",
    "    else:\n",
    "        break\n",
    "    _ += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import scipy.optimize\n",
    "import string\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ignore capitalization and remove punctuation\n",
    "### With stemming\n",
    "\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "for d in train2:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        w = stemmer.stem(w)\n",
    "        wordCount[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem6: top frequent words are:\n",
      "(1421439, 'the')\n",
      "(858941, 'and')\n",
      "(754130, 'a')\n",
      "(716892, 'to')\n",
      "(699884, 'i')\n",
      "(622798, 'of')\n",
      "(482010, 'it')\n",
      "(420605, 'is')\n",
      "(408740, 'in')\n",
      "(370663, 'thi')\n"
     ]
    }
   ],
   "source": [
    "### Just take the most popular words...\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()\n",
    "\n",
    "words = [x[1] for x in counts[:1000]]\n",
    "words_d = {words[i]:i for i in range(1000)}\n",
    "\n",
    "print(\"Problem6: top frequent words are:\")\n",
    "for _ in range(10):\n",
    "    print(counts[_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2_extract_nD_features(dataset, dim):\n",
    "    X, y, yid =[], [], []\n",
    "    for d in dataset:\n",
    "        _ = np.zeros(dim)\n",
    "        r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "        for w in r.split():\n",
    "            if w in words_d:\n",
    "                _[words_d[w]] += 1\n",
    "        X.append(_)\n",
    "        if 'genreID' in d:\n",
    "            y.append(d['genreID'])\n",
    "        yid.append(d['user_id'] + '-' + d['review_id'] + ',')\n",
    "    return np.array(X), np.array(y), yid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, _ = p2_extract_nD_features(train2, 1000)\n",
    "vX, vy, _ = p2_extract_nD_features(valid2, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod = linear_model.LogisticRegression(C=0.001)\n",
    "mod.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 7: accuracy is: 0.5504\n"
     ]
    }
   ],
   "source": [
    "pred = mod.predict(vX)\n",
    "_num_err = 0\n",
    "for _ in range(len(vy)):\n",
    "    if pred[_] != vy[_]:\n",
    "        _num_err += 1\n",
    "print('Problem 7: accuracy is: ' + str(1 - _num_err / len(vy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./assignment1/test_Category.json.gz\"\n",
    "f = gzip.open(path, 'rt', encoding=\"utf8\")\n",
    "test2 = [eval(l)for l in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/apple/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=0.05, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=500,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p8_dim = 8192\n",
    "p8_C = 0.05\n",
    "p8_iter = 500\n",
    "\n",
    "words = [x[1] for x in counts[:p8_dim]]\n",
    "words_d = {words[i]:i for i in range(p8_dim)}\n",
    "\n",
    "X, y, _ = p2_extract_nD_features(train2, p8_dim)\n",
    "vX, vy, _ = p2_extract_nD_features(valid2, p8_dim)\n",
    "tX, ty, tid = p2_extract_nD_features(test2, p8_dim)\n",
    "\n",
    "mod = linear_model.LogisticRegression(C=p8_C, max_iter=p8_iter)\n",
    "mod.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem 8: after tuning, accuracy is: 0.6856\n"
     ]
    }
   ],
   "source": [
    "pred = mod.predict(vX)\n",
    "_num_err = 0\n",
    "for _ in range(len(vy)):\n",
    "    if pred[_] != vy[_]:\n",
    "        _num_err += 1\n",
    "print('Problem 8: after tuning, accuracy is: ' + str(1 - _num_err / len(vy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate file\n",
    "pred = mod.predict(tX)\n",
    "\n",
    "predictions = open(\"predictions_Rate.csv\", 'w')\n",
    "predictions.write('userID-reviewID,prediction\\n')\n",
    "for _ in range(len(pred)):\n",
    "    predictions.write( tid[_] + str(pred[_])+'\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Again, my kaggle name is Xincheng Shen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
